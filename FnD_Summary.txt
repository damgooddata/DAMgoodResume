let me strat by telling you the story of my work at floor and decor, when i started their HQ was less than 200 people, on about 4 floors of a shared office building. There was no dress code and people would bring their dogs to work. I was hired to build dashboards and other visualizations. When i was hired the company was in the midst of massive growing pains, they were out growing every warehouse. Their sales had grown faster than they could grow their warehouses or headcount. They had a lot of data, the hope was i would build a plan to make that data more accessible. However, since it was a smaller company i was also tasked with helping with operational projects within the warehouse. Once hired, i quickly hit the road to help with the Houston warehouse expansion project. When we leased that warehouse only leased a small section of it, we had the option to lease the rest later if we needed to. So we basically just tore down a wall, then shifted everything. this was no simple undertaking, we had to essentially rebuild a warehouse without stopping its operation. The whole layout had to be re-engineered to include the new space, we built new racking, and we painted new lines. Once contractors had completed the painting and racking construction it my my teams job to put up the pic location placard for bulk locations this was hung in the air above the position, and for the racking we had to label each tier of the rack. then once labels and placard were up we were able to adjust or shift product into that space. Every night we would have to shift the warehouse so the contractors and painters could work on a new area, all the while we had ot ensure the product was correctly documented in new location within ther WMS system. they used manhattan for their wms, but i will talk about that more later. then the next day the operation would pick and ship product, then the next night we would do it all over again until all the painting and construction was ocmpleted, that was about the first 3-4 months of my career with F&D. then i returned to atlanta for a bit, i began working on the reporting changes i was hired for. the hand on operational time did give me a lot of info into manhattan and how the company worked. additionally, i was asked to take on managing the physical inventory process for each of our 4 dcs. once returning to atlanta i began researching the backend data and systems more. I had at my disposal a old desktop computer under a desk connected to a backup battery in case power went out, this was my reporting server... their databases were even worse, they were just a plethora of access databases all daisy chained together using a hodgepodge of ODBC connections and vba to build rudimentary excel reports. some of the reports were emailed some were jsut placed on network directories. i worked with IT to ensure these ODBC were available in SQL server databases, and i began to transition the reports from access databses to SQL. the struggle was the majority of the reporting required user input data, stuff the user would manually update, this would usually be data from a system they could not access via the odbc database connection in access, i worked with IT to migrate these tables to a databse i could access. A large struggle was getting systematic access to report data from manhattan, IT only wanted me to access this data from congos reports, which cognos is ok but it does not lend to analytics from scratch, it is a great tool to display data you know is valuable, i was trying to make value in the data so that i could get it to a point where it could be built into cognos. Over the next few months i began working with other teams in supply chain like the compliance team who managed packaging and pallet standard, which was important for product damage and product stackability in the warehouse. I also worked with international logistics and domestic logistics. For international i was able to stream line their freight accrual process, were were an FTZ so it was important we knew the actual product cost compared to the landed cost. we used freight stadard for this, each SKU had a cost for each vnedor - dc- store movement, and for all the movement accessorial cost like fuel and demurage. We also set up systematic reporting for tarrif management early on so when politcal climates changed we are able to quickly idenity areas impacted by tarrifs and deversify by finding factories in other countries to make the products. i was able to help the packaging and compliance teams by providing reporting on new vendors and products so they could audit correctly. however my main focus was warehouse opperations effeciency and reporting. I worked closely with compliance to develop packaging standard for pallet optmization, in cases adjusting dimensions of a pallet or how it was stacked so that we could fit more product in a bay. When i began managing the physical inventory of the warehouses it was very manual. the manhattan software had a physical inventory applet in the WMS but it was not great, it mainly manages the record keeping of the process, it did nothing to optimize the whole process. When i started we would iterate through round after round of printing hundred of sheets with product locations and information, we had pallet qty, and unit qty, meaning some was count how many pallets in a bay, and other were count how many boxes or pieces are in a bay. the pallet qty was typically floor stacked while the unit qty was on racking or shelves. when counting the pallet qty users would simply walk the floor and count by location. in reconcoliation rounds if a discrepancy was found pallet drivers would seperate pallet stacks for accessibility to count everything easier. The unit qty racking required a cherry/order picker MHE to access all the level and locations. in both cases users had their MWS enabeled scanner which they would scan at the location and then enter the qty counted. there were multiple prompts depending on if it was pallet qty (bulk) or unit qty (pick and pack). this would help the system determin if the user was counting pallets, boxes, or individual units. Originally this process took nearly a week, to complete multiple days counting and recounting locations till hour hours of the night until the variance was within an acceptable range. After the first physical inventory event in our Miami warehouse i promised myself i would never go through that again, the next ware house i was ready, i worked with IT and set upa databse containing the results of the physical inventory process, the data collected by the wms program when the users keyed into their scanners the qty and the location. I took this data, so what realtime, they refresh the table every 15 minutes. so every 15 minutes i would reconcile the snapshot of inventory prior to starting but after the inventory was locked (meaning no more inventory movement between locations, all open task had been compelted or cleared). I reconciled what was expected in each location and what was counted. I was able to radio employee and tell them they needed to go back and recount a bin when they were still close because i could tell there was either a true variance or they had keyed the count into the device incorrectly (usually this was where they counted eaches instead of boxes, the system was expecting the default value and that how it did its math so if the user entered a more granular value it would mess up the count) in my reporting i had a formula that would tell me the error, i knew based on the math, if the case qty and box qty division aligned i knew what the user meant to communicate and what the system expeceted so i knew what was wrong. in the end my realtime reporting was instumental in streamlining the process into 2 business days max. the first day was the first count. our warehouses averaged more than 1 million sqft each, with the largest being 1.4 million sqft. as you can imagine that is a lot of product to count, the second day we did a few final reconcilations but typically we were pulling orders again by lunch time. as you can imagine for a company the size of floor and decor a warehouse being offline for more than a day is a big deal, and my reporting prevented that from happening! in the end i was able to hand off my reporting tools to IT for them to build a modual directecly into the manhattan WMS. In another instance we had to move a warehouse, our LA warehouse, and similar to the physical inventory project it would take the warehouse offline for several days. I built a supplemental WMS application to manage the move. I built an application that allowed users to track the product as it was moved from the source warehouse to the destination. we were not able to use the manhattan wms since they had decided early on that they did not want to change the warehouse number. i developed a software that would allow 10 users to manage mobile scanning stations. each of these users would cover 4-5 bay doors each, a team of forklift drivers would be moving product as fast as tehy safetly could from its locatoin in the warehouse to a staging locatoin in front of each door, then the scanners would scan each pallet barcode, the program had the manifest of inventory, it would add each barcode to a queue and total the weight for all of the pallets in the queue. once the weight reached 24,000 lbs they would get an alert and they would flag that bay as ready to load, they would then move on to start scanning the next bay. the load team would begin loading the trailer with the product. then once completed with laoding the scanner person would come back to the back to the bay to realease the trailer for the yard jockey to replace with a new empty one. the scnaner would scan two seals on leave them on the back of the truck so that when the driver pulled the trailer out they could close and seal the load. then the jockey would park the trialer in the loaded section of the yard. as drivers came to pick up a load the check in employee used the program to capture the drivers info and capture a photo of thier ID, we would also provide them with a driver ID barcode card. They would present this card at destination to scan in. once checked in the drivers were directed to go pick up any trailer from the loaded section, then returnt to the gate to check out, when checking out the gate employee would scan the barcode on the seal and then the driver card and the program would print up BOL and packing list for driver and then send the driver to the destination. at the destination users would scan the seal again and the driver card to check in, they would drop the trailer in the yard and return with an empty that had already been unloaded, once the product was unloaded it would be scanned into the manhattan wms system and then my program would know it had been put away, it was a hugely successful move with almost nothing getting lost in the process! For Domestic logistics i built an invoice automation process that allowed vendors to submit invoices for valid charges and get payment very quickly. In my program vendors were required to send invoices to a generic outlook inbox with the invoice attached, the invoice was required to be in a very specific format, the program would reject theinvoice if even one line was not formatted correctly, it would send the vendor a response letting them know thier invoice had been rejected and why. If no issues the invoice was consumed and processed, to process the invoice was compared to internal customer orders and if a match was found the charge was coded for payment, the General ledger code varried greatly depending on what type of customer order it was for. but i used an algorithm to help determin the correct GL code to use. once all that was done the logistics manager got a report of all the charges and how they were being coded, they had the ability to make changes to the GL code or reject something from being paid, but if they did not do anything by the end of the fiscal week the charges were sent to AP for payment, AP would simply copy and past these entries into their system and the vendor would get paid. this saved the logistics team from having to hire a individual to manage the invoice for all of the store purchases that resulted in home delivery. I also used microsfot power automate to create a approval process for freight claims, using forms for intake and flows to may the item through the whole process. For the merchandising team designed fixed rate vendor defect program, instead of chasing down every defective product claim from a store, we dealt with vendors all over the world so actually returning the product was a pain in the butt! so instead i initiated a program with our larger vendors to document the average occurance rates for thier product, including photos and documentation of the packaging and description of the defects. using this data i negotiated defective rates with these vendors, so instead of chasing claims for defects we simply disposed of the product and billed the vendor a percentage of each dollar we spent with them that qtr, this allowed the store to not haveto complete a lot of paperwork for every time a defect was noticed. I eventually rolled out the defect program to almost every vendor, and i eventually developed a cluster model for identifying instances of defect that happened in issolatoin. When these occurred i had store opperations investigate, they were usually instances were stores had adjsuted something as defect incorrectly or they were trying to hide something they had damaged. I worked hard with oversight to ensure product adjsuted throughout the year as defect was truely defect becasue at the end of every year i did an analysis of the current defect rate vs what was actaully inccured that year and would renegotiate with the vendor if a deficit was discovered.  In addition to the vendor defect program i also set up several over cost share program with our vendors to offset the cost of business items. for things like product photo shoots for web and ecomerce stuff, or display boards for the product at the stores, we even charged a surcharge when a product was featured in a new stores grand opening. I built python based applications to manage all these subsidy program. the program would use run each month, it would be kicked off by a email from our IT system, it was systematic control file that containedtwo tabs a summary tab and a raw detail tab. the raw detail tab contined the detail of every PO and SKU paid for that period, the summary was the count and total dollar value. my program would use the attachements to aggergate how much each vendor owed for each subsidy program, most of the program only ran qtrly, but a few were monthly, like the late po chargeback. i would send all pos paid that period to the merchants with indicators if the PO was received on time or not and the  contractual late fee, the merchant could decide to waive the fee all together or charge more or less than the contractual rate. my program would consume thier responses and proceed accordingly. my program would generate the audit file on a sharepoint folder and the merchants would make their changes directly on these files. at the end of the review peroid the program would consume the repsonse and bill the vendors accordingly it would send them emails and links to a sharpoint folder with the remittance for all of their subsidy charges. The access for each of these directories was also managed through the program, it would use the data in our PIM product info management system (STIBO) to know who should have access for each vendor, even if they shared the link only user with access in PIM could access the remittance sharepoint direcories. when users were added or removed from the PIM system they would receive an email letting them know they were added or removed. the qtrly programs would look at all the POs recieved that qtr and calulate how much each vendor owed for each program for each line of each PO, the remittance was very detailed, but in some cases this was millions of vendor dollars we were taking back so it was important to be accurate and tranparent. the wtrly also had several programs that required merchant repsonse to know what to bill, similar to the late po audit, we are talking about more than 10,000 emails sent from this program with invoice to vendors. once all of the vendor has been comunicated with and all of the remittance files placed in the sharepoint folders AP recieved a consolidated files with all charges for that period they would copy and paste that into their payment system and the vendors next PO payment would be reduced by the amount owed. prior to me taking on this role and building this application it was someones full time job to calculate the subsidy charges build the excel remittance files and email to each vendor manaully. I also managed all other financial replrting for the merchant team. I built a quote tool for the pro sales team who would sale to major construction jobs like hotels or schools, this team needed to be able to give big discounts without impacting margin. since our margins varried so much depending on the product it was hard to set a range, a marble tile has a lot different margin than a painted glass tile... my tool allowed the user to quote the customers and it would only allow the user to give up so many basis points as discounts to the pro customers, the tool also would summarize the items and cost entered on the worksheet into a formal quote for the customer. however the subsidy programs took most my time as they resulted in more than 25 million in profit my last year at the company, not sales but profit! I was with F&D from a small company to a growing public traded company who in our hay day were opening 2-3 new stores every week. and my applications and reporting were instramental in the company being able to grow at that rate. i need to add that while on the supply chain team I was hired primarily for reporting and dashboards, however due to the lack of systematic data (since they were using access databases on someones computer) i had to focus on ELT work to get the data staged in data cubes, i was able to hire an employee to focus explicitly on the dashboards. all of the special warehouse move and physical inventory projects were simply a result of working an a growing start up like company, we did whatever it took to get the job done. which you can clearly see in the public success of floor and decor through these years. i did work some with SSIS while at floor and decor i primarily used Talend open sourced ETL tool as it was easier navigate without robust system permissions. I always worked on the business end of the things, not the IT side, i had to fight for permissions all the time, i was delivering enterprise level applicatoins but i had to fight for the rights needed for these applications. It took me almost a full year to get approval for a group managed service account (GMSa) that i could use to managed my deployed application, prior to that i would have to hardcode my credentials into the program sometimes and then go update it when my password changes....


 this is more information about the manufacturer defect program, including the remmediation process to clean or remmediate deffective product to make it sellable. i managed all store shrink, inlcuding manufacturer defects, carrier freight claims, and in some cases full chain of custody for items physically returned. Make sure to discuss how i used a cluster model to identify outliers in what was adjusted at the stores. especially for manufacturer defects, it typically never happened in a bubble. You are talking abut large scale production of stone and tile flooring, so if there was a color or quality issue, it would typically be the whole production batch or section of the quary where the stone was sourced. in any case it rarley ever happened in isolation, meaning it would show up in multiple places in the supply chain. Additionally, due to the nature of the product, most of it could be remediated if the damage was mold, dirt, dust or something to that effect. i also had a process built to communicate the remediation issue ot the vendor, get approval for the remmediation cost and confirm with vendor once complete and bill them through our AP department. You can even talk about how i turned the manufacturer defect bucket into a profit, i set up the vendor defective rate, this would bill a set percentage of each dollar invoiced to cover the average manufacturer defect incurred. For larger container size or high dolar issues, i would give the vendor the ability to pay for these outside the defect program, this would result in them sending us a additional payment for the daffective product or cost to remmediate. If the vendor chose to bill through the fixed rate program (x% of invoices) then this would impact their contractual rate for the following year. at the end of each fiscal year i would  complete a true up to look at the net amount stores adjusted to defective, and the net invoiced cost, the resulting percentage would be their deffective rate the following year. so in instances of one out outlying situations it was in their best interest to pay outside the program. I used the cluster model to hold the stores accountable for everything they adjsuted. once product was adjusted in the ERP my program would see it and run it through the model, they in turn would get a systematic response (or they could manualy query the ERP to see) indicating if thier adjustment required further information. Basically the two most outlying quadrants would require feedback or response from the store who adjusted the product to provide validation of the defect to ensure the stores were not incorrectly adjusting product to manufacturer defect instead of damaged, the damaged bucket indicated the damage was caused by the store and went against the stores budget. the manufacturer defect did not, they were a corperate account and the store did not directly have to account for this cost in their budget, so they would sometime try to hide mistakes so they did not impact their bonuses. but the cluster model was cutting edge for its time, this was back in 2021. This was a huge win for store opperations, it saved them a ton of time operationally as the model ran every 15 minutes. so they stores did not have to manage a lot of documentation and the product did not clog up thier back room waiting on a resolution. it was very quick and the stores typucally tossed the product in the dumpster. the only gap in the program is the expense for the additional dumpster tonnage and pickup frequency was not something i was able to pass to the vendors even though i could easily quantify it based on the product information for the items being disposed of.

more info about manufacturer defect program:
"Loss Prevention Cluster Model". here is a synopsis of this task. I was helping to set up reporting to manage the vendor and freight carrier claims process for a major retailer with more than 200 stores across the country. The majority of the product was sourced and manufactured overseas and shipped in container qty, so returning product to a vendor was not really an option, due to the nature of the product there was no value to it once damaged. the 200 stores were serviced by 4 warehouses. if defective product was found at a warehouse or store, the employee would adjust the product in the ERP system, then submit photos and documentation to a internal website. Originally the company would have to work with the vendor for repayment for each instance incurred, this was obviuosly very time consuming. And in the end the product was disposed of since there was no value. The warehouse and stores also would incur internal damage from employee handling errors. They were held accountable for the damage but not the vendor defect, since they had no impact on the vendor defect. I developed a plan to document the average rate of defect for each vendor. This rate was adjusted annually to align with the past fiscal period actual defect rate.  Since the vendors no longer needed to the documentation for each defective instance, there was no longer a need for the stores or warehouses do all the paper work. however this opened up the opportunity for savvy employees to adjust damaged product as vendor defect to bypass accountability. I developed a cluster model to map employee behavior. Due to the nature of the product it was extremly unlikely for defects to occur in a bubble. meaning the defect would always be see in more than one place and average qty accross the supply chain. when stores would be the only to adjsut something, or the adjust a exuberant amount + or - from other locations the model would identify these behaviors and flag these adjustment transactions for manager review. The manager for the store or warehouse would have to investigate these situations manually and report back to corporate, if it was valid that was it, but if not vendor defect the adjustment would be backed out and then adjusted into damage. Additionally when there was a large scale issue, maybe a whole contianer was damaged when arrived at the warehouse, the program would notify the vendor and allow them to pay for that situation outside of the fixed rate program, since the prior years actual adjusted $/ sales$ was the next year's new defctive rate, so it was in their best interest. This turned a very slow manual fustrating process into a turn key disposal and accountability process. Prior to my program stores would be constrained for space in the back room because they would have to store defective product while waiting on response form the vendor. whith my program they simply disposed of defective prodcut when incurred. Additonally they no longer had to complete the tedious task of submitting photos and documentation. this allowed them to focus on serving the customer more! and protected the company from potentially loosing money due to employee mistakes (intentional or not)  